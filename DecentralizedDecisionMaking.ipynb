{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN5rIoHcU//65y748lyZJGo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evanss2025/Exploring-Slime-Mold-Inspired-Strategies-for-Improved-Domain-Adaptation-for-Visual-Recognition-in-ML/blob/main/DecentralizedDecisionMaking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exploring Slime Mold-Inspired Strategies for Improved Domain Adaptation for Visual Recognition in ML**\n",
        "\n",
        "This code is a part of Sophia Evans's research project. The following creates a domain shift situation in which the SVHN dataset is used to train and validate a classification model, but then the model predicts on the MNIST dataset. This creates domain shift as although both are number datasets, the SVHN dataset introduces new lighting and features that the MNIST dataset does not have, limiting it's efficiency."
      ],
      "metadata": {
        "id": "yWvkGCm-CQAx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing libraries"
      ],
      "metadata": {
        "id": "I5G92GnECQAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JaAVUBr4CQAy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Downloading the SVHN dataset. The dataset is downloaded from tensorflow's datasets and the appropriate feautres are extracted from it."
      ],
      "metadata": {
        "id": "6FzFc131CQAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u8Odv8bpCQAy"
      },
      "outputs": [],
      "source": [
        "# Load SVHN dataset\n",
        "svhn_builder = tfds.builder('svhn_cropped')\n",
        "svhn_builder.download_and_prepare()\n",
        "svhn_dataset = tfds.load('svhn_cropped', split='train', as_supervised=True)\n",
        "svhn_dataset = tfds.as_numpy(svhn_dataset)\n",
        "\n",
        "# Extract features and labels from SVHN dataset\n",
        "x_svhn = np.array([tf.image.rgb_to_grayscale(sample[0]).numpy().astype('float32') / 255 for sample in svhn_dataset])\n",
        "y_svhn = np.array([sample[1] for sample in svhn_dataset])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing the SVHN dataset"
      ],
      "metadata": {
        "id": "i4mjctrICQAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "APsTxNivCQAy"
      },
      "outputs": [],
      "source": [
        "# Split data into train and test sets for the source domain (SVHN)\n",
        "source_train_data, source_test_data, y_svhn_train, y_svhn_test = train_test_split(\n",
        "    x_svhn, y_svhn, test_size=0.2, random_state=42, shuffle=True\n",
        ")\n",
        "# Ensure target labels have the same shape as source labels\n",
        "y_svhn_train = tf.keras.utils.to_categorical(y_svhn_train, num_classes=10)\n",
        "y_svhn_test = tf.keras.utils.to_categorical(y_svhn_test, num_classes=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading the MNIST dataset from Keras, and then preprocessing the dataset."
      ],
      "metadata": {
        "id": "nLfzMieHCQAy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nZKf8jjCQAz"
      },
      "outputs": [],
      "source": [
        "# Load MNIST dataset\n",
        "mnist = tf.keras.datasets.mnist\n",
        "(x_mnist, y_mnist), (_, _) = mnist.load_data()\n",
        "\n",
        "# Resize MNIST images to 32x32 pixels\n",
        "x_mnist_resized = tf.image.resize(x_mnist[..., tf.newaxis], (32, 32))\n",
        "x_mnist_resized = tf.squeeze(x_mnist_resized)\n",
        "\n",
        "# Preprocess MNIST data\n",
        "x_mnist_resized = x_mnist_resized.numpy()  # Convert to NumPy array\n",
        "x_mnist_resized = x_mnist_resized.reshape((x_mnist_resized.shape[0], 32, 32, 1)).astype('float32') / 255\n",
        "y_mnist = tf.keras.utils.to_categorical(y_mnist, num_classes=10)\n",
        "\n",
        "# Split data into train and test sets for the target domain (MNIST)\n",
        "target_train_data, target_test_data, target_train_labels, target_test_labels = train_test_split(\n",
        "    x_mnist_resized, y_mnist, test_size=0.2, random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualizing an example from each dataset ensuring that they are both the same size and are compatible."
      ],
      "metadata": {
        "id": "JA6ac6dRCQAz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLhHaPUzCQAz"
      },
      "outputs": [],
      "source": [
        "# Visualize an example image from each dataset\n",
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "# SVHN\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.imshow(source_train_data[0].reshape(32, 32), cmap='gray')\n",
        "plt.title('SVHN Sample')\n",
        "\n",
        "# MNIST\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.imshow(target_train_data[0], cmap='gray')\n",
        "plt.title('MNIST Sample')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the model architecture, adding regularization and dropout parameters, compiling the model, and then train the model on the SVHN dataset\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "vr4i6Dw1CQAz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "umjGsUf0CQAz"
      },
      "outputs": [],
      "source": [
        "# Adding parameters\n",
        "dropout_rate = 0.2\n",
        "l2_regularization = 1e-4\n",
        "\n",
        "# Define the model\n",
        "model = models.Sequential()\n",
        "\n",
        "# Convolutional layers\n",
        "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 1)))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation='relu'))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "model.add(tf.keras.layers.Dropout(dropout_rate))\n",
        "model.add(tf.keras.layers.Dense(10, kernel_regularizer=tf.keras.regularizers.l2(l2_regularization)))\n",
        "\n",
        "# Flatten layer\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Dense layers\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "\n",
        "# Output layer\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "# Display the model summary\n",
        "model.summary()\n",
        "\n",
        "# Compile the model with regularization\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# Train on the source domain (SVHN) with regularization\n",
        "model.fit(source_train_data, y_svhn_train, epochs=10, validation_data=(source_test_data, y_svhn_test), batch_size=32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applying the Decentralized Decision Making Strategy"
      ],
      "metadata": {
        "id": "PS_G6COsCccd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Subsample the target domain data\n",
        "subsample_percentage = 0.1\n",
        "subsample_size = int(subsample_percentage * len(target_train_data))\n",
        "target_train_data_subsample = target_train_data[:subsample_size]\n",
        "target_train_labels_subsample = target_train_labels[:subsample_size]\n",
        "\n",
        "# Pseudo-labeling on the subsampled target domain\n",
        "target_predictions = model.predict(target_train_data_subsample)\n",
        "\n",
        "# Filter confident predictions using both threshold and entropy\n",
        "confidence_threshold = 0.2\n",
        "entropy_threshold = 0.6\n",
        "entropy = -np.sum(target_predictions * np.log(target_predictions), axis=1)\n",
        "\n",
        "# Apply decentralized decision-making strategy\n",
        "print_interval = 100\n",
        "num_target_samples = len(target_train_data_subsample)\n",
        "\n",
        "# Inside the training loop\n",
        "for i in range(num_target_samples):\n",
        "    if np.max(target_predictions[i]) > confidence_threshold and entropy[i] < -np.log(entropy_threshold):\n",
        "        # Update the model based on the individual sample with regularization\n",
        "        sample_data = target_train_data_subsample[i:i+1]\n",
        "        sample_label = np.expand_dims(tf.keras.utils.to_categorical(np.argmax(target_predictions[i]), num_classes=10), axis=0)\n",
        "        history = model.fit(sample_data, sample_label, epochs=1, batch_size=1, verbose=0)\n",
        "\n",
        "        if (i + 1) % print_interval == 0 or (i + 1) == num_target_samples:\n",
        "            print(f\"Processed {i + 1}/{num_target_samples} samples - Loss: {history.history['loss'][0]}, Accuracy: {history.history['accuracy'][0]}\")"
      ],
      "metadata": {
        "id": "dCjcMiB7CZM_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The SVHN trained model predicts on the MNIST dataset and outputs the accuracy and F1-score"
      ],
      "metadata": {
        "id": "hx9uWZIp_EQ9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "db5-D7-zlV2O"
      },
      "outputs": [],
      "source": [
        "# Evaluate on the target domain\n",
        "target_predictions = model.predict(target_test_data)\n",
        "target_accuracy = accuracy_score(np.argmax(target_test_labels, axis=1), np.argmax(target_predictions, axis=1))\n",
        "target_f1_score = f1_score(np.argmax(target_test_labels, axis=1), np.argmax(target_predictions, axis=1), average='weighted')\n",
        "\n",
        "print(f\"Target Domain Accuracy: {target_accuracy}\")\n",
        "print(f\"Target Domain F1 Score: {target_f1_score}\")"
      ]
    }
  ]
}